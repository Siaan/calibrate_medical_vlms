{"cells":[{"cell_type":"code","execution_count":null,"id":"ubaUyNC9aOZA","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54559,"status":"ok","timestamp":1713517953154,"user":{"displayName":"Tom Zollo","userId":"03927008826524010748"},"user_tz":240},"id":"ubaUyNC9aOZA","outputId":"9ec601aa-3da4-474a-98ed-d777be77ccad"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","\n","drive.mount(\"/content/drive\")"]},{"cell_type":"code","execution_count":null,"id":"a5ZU0yN2Zbqz","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":76253,"status":"ok","timestamp":1713518029405,"user":{"displayName":"Tom Zollo","userId":"03927008826524010748"},"user_tz":240},"id":"a5ZU0yN2Zbqz","outputId":"fe10e11f-40d1-4773-cd93-d7a10790ffea"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for uncertainty-calibration (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q uncertainty-calibration open_clip_torch"]},{"cell_type":"code","execution_count":null,"id":"ui-lBiQ1Z-eE","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15584,"status":"ok","timestamp":1713518044982,"user":{"displayName":"Tom Zollo","userId":"03927008826524010748"},"user_tz":240},"id":"ui-lBiQ1Z-eE","outputId":"bfd9e660-693f-4afe-cfbc-05c837b2444f"},"outputs":[{"name":"stdout","output_type":"stream","text":["  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install -q git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":null,"id":"x6-zE5sn27Ik","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12454,"status":"ok","timestamp":1713518057429,"user":{"displayName":"Tom Zollo","userId":"03927008826524010748"},"user_tz":240},"id":"x6-zE5sn27Ik","outputId":"4fd8aa72-2fe7-43e9-919d-98e2ccf9e2d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q datasets"]},{"cell_type":"code","execution_count":null,"id":"11b41e22-96e8-484c-89ed-b5382adfc3f4","metadata":{"id":"11b41e22-96e8-484c-89ed-b5382adfc3f4"},"outputs":[],"source":["import random\n","import os\n","from dataclasses import dataclass\n","import glob\n","import pathlib\n","from tqdm import tqdm\n","from PIL import Image\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Subset\n","from torch.nn import functional as F\n","from torchvision import transforms\n","import torchvision\n","from torchvision.datasets import ImageFolder\n","from transformers import AutoProcessor, CLIPModel, AutoTokenizer\n","from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize, InterpolationMode, Lambda\n","import calibration as cal\n","import open_clip\n","import clip\n","from datasets import load_dataset\n","\n","import matplotlib.pyplot as plt\n","from matplotlib import style\n","plt.style.use('seaborn-v0_8')"]},{"cell_type":"code","execution_count":null,"id":"zBLgwt9FZVKp","metadata":{"id":"zBLgwt9FZVKp"},"outputs":[],"source":["data_root = \"drive/MyDrive/CV2_project/data/\"\n","\n","MHIST_DIR = data_root+'mhist/'\n","PCAM_DIR = data_root+'pcam/'\n","LCLUNG_DIR = data_root+'lung_colon_image_set/lung_image_sets/'\n","LCCOLON_DIR = data_root+'lung_colon_image_set/colon_image_sets/'\n","BACH_DIR = data_root+'ICIAR2018_BACH_Challenge/Photos/'\n","NCK_DIR = data_root+'NCT-CRC-HE-100K/'\n","SICAPv2_DIR = data_root+'SICAPv2/'\n","SKIN_DIR = data_root+'SkinCancer_Files/data/'\n","OSTEO_DIR = data_root+'Osteosarcoma_Tumor_Assessment/Separated/'\n","RENAL_DIR = data_root+'tissue_classification/'\n","DATABIOX_DIR = data_root+'databiox/'\n","TUMOR_DIR = data_root+'SkinTumor/'\n"]},{"cell_type":"code","execution_count":null,"id":"a0290462-97ac-4055-9fe7-1131ac03b81f","metadata":{"id":"a0290462-97ac-4055-9fe7-1131ac03b81f"},"outputs":[],"source":["all_dataset_class_labels = {\n","    \"pcam\": [\n","        \"lymph node\",\n","        \"lymph node containing metastatic tumor tissue\"\n","    ],\n","    \"nck\": [\"adipose\",\n","            \"debris\",\n","            \"lymphocytes\",\n","            \"mucus\",\n","            \"smooth muscle\",\n","            \"normal colon mucosa\",\n","            \"cancer-associated stroma\",\n","            \"colorectal adenocarcinoma epithelium\"\n","    ],\n","    \"lc25000_lung\": [\"benign lung\",\n","                     \"lung adenocarcinoma\",\n","                     \"lung squamous cell carcinoma\"\n","    ],\n","    \"lc25000_colon\": [\"colon adenocarcinoma\",\n","                      \"benign colonic tissue\"\n","    ],\n","    \"mhist\": [\"hyperplastic polyp\",\n","              \"sessile serrated adenoma\"\n","    ],\n","    \"sicap\": [\"benign glands\",\n","        \"atrophic dense glands\",\n","        \"cribriform ill-formed fused papillary patterns\",\n","        \"isolated nest cells without lumen roseting patterns\"\n","    ],\n","    \"idc_grade\": [\"well differentiated bloom richardson grade one\",\n","                \"moderately differentiated bloom richardson grade two\",\n","                \"poorly differentiated grade three\"\n","    ],\n","    \"databiox\": [\"well differentiated bloom richardson grade one\",\n","                \"moderately differentiated bloom richardson grade two\",\n","                \"poorly differentiated grade three\"\n","    ],\n","    \"osteo\": [\"non-tumor\",\n","        \"non-viable necrotic osteosarcoma tumor\",\n","        \"viable osteosarcoma tumor\"\n","    ],\n","    \"bach\": [\"breast non-malignant benign tissue\",\n","            \"breast malignant in-situ carcinoma\",\n","            \"breast malignant invasive carcinoma\",\n","            \"breast normal breast tissue\"],\n","    \"renal_cell\": [\"red blood cells\",\n","                     \"renal cancer\",\n","                     \"normal renal tissue\",\n","                     \"torn adipose necrotic tissue\",\n","                     \"muscle fibrous stroma blood vessels\"\n","    ],\n","    \"skin\": [\"necrosis\",\n","        \"skeletal muscle\",\n","        \"eccrine sweat glands\",\n","        \"vessels\",\n","        \"elastosis\",\n","        \"chondral tissue\",\n","        \"hair follicle\",\n","        \"epidermis\",\n","        \"nerves\",\n","        \"subcutis\",\n","        \"dermis\",\n","        \"sebaceous glands\",\n","        \"squamous-cell carcinoma\",\n","        \"melanoma in-situ\",\n","        \"basal-cell carcinoma\",\n","        \"naevus\"\n","    ],\n","    \"skin_tumor\": [\n","        \"squamous-cell carcinoma\",\n","        \"melanoma in-situ\",\n","        \"basal-cell carcinoma\",\n","        \"naevus\"\n","    ]\n","}"]},{"cell_type":"code","execution_count":null,"id":"-XBFUORsbIJm","metadata":{"id":"-XBFUORsbIJm"},"outputs":[],"source":["class MhistDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, csv_file, image_dir, transform=None, train=True):\n","        csv_file = os.path.join(root, csv_file)\n","        image_dir = os.path.join(root, image_dir)\n","\n","        self.data = pd.read_csv(csv_file)\n","        # if train:\n","        #     self.data = self.data[self.data['Partition'] == 'train']\n","        # else:\n","        #     self.data = self.data[self.data['Partition'] != 'train']\n","        self.image_paths = self.data['Image Name'].values\n","        self.labels = self.data['Majority Vote Label'].values\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.train = train\n","        self.cat_to_num_map = {'HP': 0, 'SSA': 1}\n","        self.classes = [\"hyperplastic polyp\", \"sessile serrated adenoma\"]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        image_path = os.path.join(self.image_dir, self.image_paths[index])\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.cat_to_num_map[self.labels[index]]\n","\n","        return image, label\n","\n","\n","class SicapDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, image_dir, transform=None, train=True):\n","\n","        image_dir = os.path.join(root, image_dir)\n","\n","        if train:\n","            csv_file = os.path.join(root, \"partition/Test\", \"Train.xlsx\")\n","            self.data = pd.read_excel(csv_file)\n","        else:\n","            csv_file = os.path.join(root, \"partition/Test\", \"Test.xlsx\")\n","            self.data = pd.read_excel(csv_file)\n","\n","        # drop all columns except image_name and the label columns\n","        label_columns = ['NC', 'G3', 'G4', 'G5']  # , 'G4C']\n","        self.data = self.data[['image_name'] + label_columns]\n","\n","        # get the index of the maximum label value for each row\n","        self.data['labels'] = self.data[label_columns].idxmax(axis=1)\n","\n","        # replace the label column values with categorical values\n","        self.cat_to_num_map = label_map = {'NC': 0, 'G3': 1, 'G4': 2, 'G5': 3}  # , 'G4C': 4}\n","        self.data['labels'] = self.data['labels'].map(label_map)\n","\n","        self.image_paths = self.data['image_name'].values\n","        self.labels = self.data['labels'].values\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.train = train\n","        self.classes = [\"non-cancerous well-differentiated glands\",\n","                        \"gleason grade 3 with atrophic well differentiated and dense glandular regions\",\n","                        \"gleason grade 4 with cribriform, ill-formed, large-fused and papillary glandular patterns\",\n","                        \"gleason grade 5 with nests of cells without lumen formation, isolated cells and pseudo-roseting patterns\",\n","                        ]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        image_path = os.path.join(self.image_dir, self.image_paths[index])\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.labels[index]\n","\n","        return image, label\n","\n","\n","class ArchCsvDataset(torch.utils.data.Dataset):\n","    def __init__(self, csv_file, transforms, img_key='image_path', caption_key='caption', sep=\",\"):\n","        df = pd.read_csv(csv_file, sep=sep)\n","        self.images = df[img_key].tolist()\n","        self.captions = df[caption_key].tolist()\n","        self.transforms = transforms\n","        self.ids = list(sorted(df['ids'].tolist()))\n","\n","    def __len__(self):\n","        return len(self.captions)\n","\n","    def __getitem__(self, idx):\n","        id_ = self.ids[idx]\n","        images = self.transforms(Image.open(str(self.images[id_])))\n","        texts = [str(self.captions[id_])]\n","        return images, texts\n","\n","\n","class OsteoDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, csv_file, image_dir, transform=None):\n","        csv_file = os.path.join(root, csv_file)\n","        image_dir = os.path.join(root, image_dir)\n","\n","        self.data = pd.read_csv(csv_file)\n","        self.data = self.data[self.data['classification'] != \"viable: non-viable\"] #53 samples removed from 1144\n","        self.image_paths = self.data['image.name'].values\n","        self.labels = self.data['classification'].values\n","        self.image_dir = image_dir\n","        self.transform = transform\n","        self.cat_to_num_map = {'Non-Tumor': 0, 'Non-Viable-Tumor': 1, 'Viable': 2}\n","        self.classes = [\"non-tumor\", \"necrotic tumor\", \"viable tumor\"]\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        image_path = os.path.join(self.image_dir, self.image_paths[index])\n","        image_path = image_path.replace(' - ', '-')\n","        image_path = glob.glob(f\"{image_path.replace(' ', '-')}*\")[0]\n","        image = Image.open(image_path.replace(' ', '-')).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        label = self.cat_to_num_map[self.labels[index]]\n","\n","        return image, label\n","\n","\n","class SkinDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, csv_file, transform=None, train=True, val=False,\n","                 tumor=False):\n","        csv_file = os.path.join(root, csv_file)\n","        self.data = pd.read_csv(csv_file)\n","\n","        if train:\n","            self.data = self.data[self.data['set'] == 'Train']\n","        else:\n","            if val:\n","                self.data = self.data[self.data['set'] == \"Validation\"]\n","            else:\n","                self.data = self.data[self.data['set'] == 'Test']\n","\n","        if tumor:\n","            self.data = self.data[self.data['malignicy'] == 'tumor']\n","        self.tumor = tumor\n","\n","        self.image_paths = self.data['file'].values\n","        self.labels = self.data['class'].values\n","\n","        self.transform = transform\n","        self.train = train\n","\n","        self.cat_to_num_map = {'nontumor_skin_necrosis_necrosis': 0,\n","                               'nontumor_skin_muscle_skeletal': 1,\n","                               'nontumor_skin_sweatglands_sweatglands': 2,\n","                               'nontumor_skin_vessel_vessel': 3,\n","                               'nontumor_skin_elastosis_elastosis': 4,\n","                               'nontumor_skin_chondraltissue_chondraltissue': 5,\n","                               'nontumor_skin_hairfollicle_hairfollicle': 6,\n","                               'nontumor_skin_epidermis_epidermis': 7,\n","                               'nontumor_skin_nerves_nerves': 8,\n","                               'nontumor_skin_subcutis_subcutis': 9,\n","                               'nontumor_skin_dermis_dermis': 10,\n","                               'nontumor_skin_sebaceousglands_sebaceousglands': 11,\n","                               'tumor_skin_epithelial_sqcc': 12,\n","                               'tumor_skin_melanoma_melanoma': 13,\n","                               'tumor_skin_epithelial_bcc': 14,\n","                               'tumor_skin_naevus_naevus': 15\n","                               }\n","\n","        self.tumor_map = {'tumor_skin_epithelial_sqcc': 0,\n","                          'tumor_skin_melanoma_melanoma': 1,\n","                          'tumor_skin_epithelial_bcc': 2,\n","                          'tumor_skin_naevus_naevus': 3\n","                          }\n","\n","        self.classes = list(self.cat_to_num_map)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        image_path = self.image_paths[index]\n","        image = Image.open(image_path).convert('RGB')\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        if not self.tumor:\n","            label = self.cat_to_num_map[self.labels[index]]\n","        else:\n","            label = self.tumor_map[self.labels[index]]\n","\n","        return image, label"]},{"cell_type":"code","execution_count":null,"id":"224b6356-912f-4872-b957-016a00fb8936","metadata":{"id":"224b6356-912f-4872-b957-016a00fb8936"},"outputs":[],"source":["def get_dataset(preprocess, args):\n","\n","    def iloader(path):\n","        image = Image.open(path)\n","        return image\n","\n","    if args.dataset == 'pcam':\n","\n","        args.data_dir = pathlib.Path(PCAM_DIR)\n","        dataset = torchvision.datasets.PCAM(\n","            root=args.data_dir,\n","            transform=preprocess,\n","            download=True,\n","            split=\"test\",\n","        )\n","\n","\n","    elif args.dataset == 'lc25000_colon':\n","\n","        args.data_dir = pathlib.Path(LCCOLON_DIR)\n","        dataset = torchvision.datasets.DatasetFolder(\n","            root=args.data_dir,\n","            loader = iloader,\n","            transform=preprocess,\n","            extensions = 'jpeg'\n","        )\n","\n","\n","    elif args.dataset == 'lc25000_lung':\n","\n","        # args.data_dir = pathlib.Path(LCLUNG_DIR)\n","        # dataset = torchvision.datasets.DatasetFolder(\n","        #     root=args.data_dir,\n","        #     loader = iloader,\n","        #     transform=preprocess,\n","        #     extensions = 'jpeg'\n","        # )\n","\n","        dataset = load_dataset(\"1aurent/LC25000\")\n","        dataset = dataset[\"train\"].filter(lambda example: example[\"organ\"] == 0)\n","        dataset = dataset.with_format(\"torch\")\n","        def ds_transforms(examples):\n","            examples[\"image\"] = [preprocess(image) for image in examples[\"image\"]]\n","            return examples\n","        dataset = dataset.with_transform(ds_transforms)\n","\n","    elif args.dataset == 'bach':\n","\n","        args.data_dir = pathlib.Path(BACH_DIR)\n","        dataset = torchvision.datasets.ImageFolder(\n","            root=args.data_dir,\n","            transform=preprocess,\n","        )\n","\n","    elif args.dataset == 'nck':\n","\n","        args.data_dir = pathlib.Path(NCK_DIR)\n","        dataset = torchvision.datasets.ImageFolder(\n","            root=args.data_dir,\n","            transform=preprocess,\n","        )\n","\n","    elif args.dataset == 'sicap':\n","\n","        args.data_dir = pathlib.Path(SICAPv2_DIR)\n","        dataset = SicapDataset(\n","            root=args.data_dir,\n","            image_dir=\"images\",\n","            transform=preprocess,\n","            train=False\n","        )\n","\n","    elif args.dataset == 'skin':\n","\n","        args.data_dir = pathlib.Path(SKIN_DIR)\n","        dataset = SkinDataset(\n","            root=args.data_dir,\n","            csv_file=\"tiles-v2.csv\",\n","            transform=preprocess,\n","            train=False,\n","            tumor=False\n","        )\n","\n","    elif args.dataset == 'skin_tumor':\n","\n","        args.data_dir = pathlib.Path(TUMOR_DIR)\n","        dataset = SkinDataset(\n","            root=args.data_dir,\n","            csv_file=\"tiles-v3.csv\",\n","            transform=preprocess,\n","            train=False,\n","            tumor=True\n","        )\n","\n","    elif args.dataset == 'mhist':\n","\n","        args.data_dir = pathlib.Path(MHIST_DIR)\n","        dataset = MhistDataset(\n","            root=args.data_dir,\n","            csv_file=\"annotations.csv\",\n","            image_dir=\"images\",\n","            transform=preprocess\n","        )\n","\n","    elif args.dataset == 'osteo':\n","\n","      args.data_dir = pathlib.Path(OSTEO_DIR)\n","      dataset = torchvision.datasets.ImageFolder(\n","          root=args.data_dir,\n","          transform=preprocess,\n","      )\n","    elif args.dataset == 'renal_cell':\n","\n","      args.data_dir = pathlib.Path(RENAL_DIR)\n","      dataset = torchvision.datasets.ImageFolder(\n","          root=args.data_dir,\n","          transform=preprocess,\n","      )\n","    elif args.dataset == 'databiox':\n","\n","      args.data_dir = pathlib.Path(DATABIOX_DIR)\n","      dataset = torchvision.datasets.ImageFolder(\n","          root=args.data_dir,\n","          transform=preprocess,\n","    )\n","\n","\n","    else:\n","        raise ValueError\n","\n","    return dataset\n","\n","\n","def get_label_texts(labels_to_classname, args):\n","\n","    label_to_texts = dict()\n","\n","    if args.descriptors is not None:\n","        if args.descriptors == \"sentence\":\n","\n","            save_path = \"drive/MyDrive/CV2_project/code/med_vlm_cal/descriptors/sentence/{}.csv\".format(args.dataset)\n","            df = pd.read_csv(save_path)\n","            for label in all_dataset_class_labels[args.dataset]:\n","                row = df[df[\"label\"] == label].iloc[0]\n","                response = eval(row[\"response\"])\n","                texts = []\n","                for c in response[\"choices\"]:\n","                    clean = c[\"message\"][\"content\"].strip().strip('\\\"')\n","                    texts.append(clean)\n","                label_to_texts[label] = texts\n","\n","        elif args.descriptors == \"feature\":\n","\n","            save_path = \"drive/MyDrive/CV2_project/code/med_vlm_cal/descriptors/feature/{}.csv\".format(dataset)\n","            df = pd.read_csv(save_path)\n","            for label in all_dataset_class_labels[args.dataset]:\n","                row = df[df[\"label\"] == label].iloc[0]\n","                response = eval(row[\"response\"])\n","                c = response[\"choices\"][0][\"message\"][\"content\"].strip()\n","                texts = [\"histopathology image of \"+label+\" with \"+t.split(\". \")[1].strip(\".\") for t in c.split(\"\\n\")]\n","                label_to_texts[label] = texts\n","\n","        else:\n","            raise ValueError\n","\n","    else:\n","        templates = [\"a histopathology slide showing {c}\",\n","                \"histopathology image of {c}\",\n","                \"pathology tissue showing {c}\",\n","                \"presence of {c} tissue on image\"]\n","        for classname in labels_to_classname:\n","            texts = [template.format(c=classname) for template in templates]\n","            label_to_texts[classname] = texts\n","\n","    print(label_to_texts)\n","\n","    return label_to_texts\n","\n","\n","def compute_label_encodings(model, tokenizer, label_to_texts, args):\n","\n","    zeroshot_weights = []\n","    for classname, texts in label_to_texts.items():\n","        print(classname, texts)\n","        if args.model in [\"plip\", \"pubmed\"]:\n","            input = tokenizer(texts, padding=True, return_tensors=\"pt\").to(\"cuda\")\n","            class_embedding = model.get_text_features(**input)\n","        else:\n","            input = tokenizer(texts).to(args.device)  # tokenize\n","            class_embedding = model.encode_text(input)\n","\n","        class_embedding = F.normalize(class_embedding, dim=-1).mean(dim=0)\n","        class_embedding /= class_embedding.norm()\n","        zeroshot_weights.append(class_embedding)\n","\n","    label_encodings = torch.stack(zeroshot_weights, dim=1).T.to(args.device)\n","\n","    return label_encodings\n","\n","\n","clip_transform = Compose([\n","            Lambda(lambda img: img.convert(\"RGB\")),\n","            Resize(size=224, interpolation=InterpolationMode.BICUBIC),              # Resize the shortest side to 256 pixels\n","            CenterCrop(224),          # Crop a square in the center with sides of length 224 pixels\n","            ToTensor(),               # Convert the image to a PyTorch tensor\n","            Normalize(mean=[0.48145466, 0.4578275, 0.40821073],  # Normalize using the mean and std dev\n","                      std=[0.26862954, 0.26130258, 0.27577711]),\n","        ])\n","\n","\n","def get_model_and_tokenizer(args):\n","\n","    if args.model == \"quilt\":\n","        model_path = \"hf-hub:wisdomik/QuiltNet-B-32\"\n","        model, _, preprocess = open_clip.create_model_and_transforms(model_path, device=args.device)\n","        tokenizer = open_clip.get_tokenizer(model_path)\n","\n","    elif args.model == \"quilt16\":\n","        model_path = \"hf-hub:wisdomik/QuiltNet-B-16\"\n","        model, _, preprocess = open_clip.create_model_and_transforms(model_path, device=args.device)\n","        tokenizer = open_clip.get_tokenizer(model_path)\n","\n","    elif args.model == \"quiltbert\":\n","        model_path = \"hf-hub:wisdomik/QuiltNet-B-16-PMB\"\n","        model, _, preprocess = open_clip.create_model_and_transforms(model_path, device=args.device)\n","        tokenizer = open_clip.get_tokenizer(model_path)\n","\n","    elif args.model == \"biomed\":\n","        model_path = \"hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224\"\n","        model, preprocess = open_clip.create_model_from_pretrained(model_path, device=args.device)\n","        tokenizer = open_clip.get_tokenizer(model_path)\n","\n","    elif args.model == \"plip\":\n","        model_path = \"vinid/plip\"\n","        model = CLIPModel.from_pretrained(model_path).to(\"cuda\")\n","        tokenizer = AutoProcessor.from_pretrained(model_path).tokenizer\n","        preprocess = clip_transform\n","\n","    elif args.model == \"pubmed\":\n","        model_path = \"flaviagiammarino/pubmed-clip-vit-base-patch32\"\n","        model = CLIPModel.from_pretrained(model_path).to(\"cuda\")\n","        tokenizer = AutoProcessor.from_pretrained(model_path).tokenizer\n","        preprocess = clip_transform\n","\n","    elif args.model == \"vitb32\":\n","        model_path = \"ViT-B-32\"\n","        model, _, preprocess = open_clip.create_model_and_transforms(model_path, device=args.device)\n","        tokenizer = open_clip.get_tokenizer(model_path)\n","\n","    else:\n","        raise ValueError\n","\n","    return model, preprocess, tokenizer"]},{"cell_type":"code","execution_count":null,"id":"4b0bb85c-613f-4f37-b39a-b812544e7a8f","metadata":{"id":"4b0bb85c-613f-4f37-b39a-b812544e7a8f"},"outputs":[],"source":["def extract_features(args):\n","    torch.manual_seed(args.seed)\n","\n","    print(\"Loading model...\")\n","\n","    device = torch.device(args.device)\n","\n","    model, preprocess, tokenizer = get_model_and_tokenizer(args)\n","\n","    model.eval()\n","\n","    dataset = get_dataset(preprocess, args)\n","    print(\"n data:\", len(dataset))\n","\n","    save_path = \"drive/MyDrive/CV2_project/code/med_vlm_cal/output/{}/{}/\".format(args.model, args.dataset)\n","    if args.descriptors is not None:\n","        save_path += \"{}_desc_\".format(args.descriptors)\n","    print(\"save to:\", save_path)\n","\n","    torch.manual_seed(args.seed)\n","    dataloader = DataLoader(\n","        dataset, args.batch_size,\n","        num_workers=2, pin_memory=True,\n","        shuffle=True\n","    )\n","\n","    class_label_text = all_dataset_class_labels[args.dataset]\n","    label_to_texts = get_label_texts(class_label_text, args)\n","\n","    with torch.no_grad():\n","\n","        label_encodings = compute_label_encodings(model, tokenizer, label_to_texts, args)\n","\n","        all_features = []\n","        all_logits = []\n","        all_labels = []\n","\n","        batch_idx = 0\n","        for batch in tqdm(dataloader):\n","\n","            if args.dataset in [\"lc25000_lung\"]:\n","                images = batch[\"image\"]\n","                labels = batch[\"label\"]\n","            else:\n","                images, labels = batch\n","\n","            images = images.to(device)\n","            labels = labels.to(device)\n","\n","            if args.model in [\"plip\", \"pubmed\"]:\n","                image_encodings = model.get_image_features(images)\n","                image_encodings = F.normalize(image_encodings)\n","            else:\n","                image_encodings = model.encode_image(images, normalize=True)\n","            image_labels_similarity = 100*image_encodings @ label_encodings.T\n","\n","            all_features.append(image_encodings.detach().cpu())\n","            all_logits.append(image_labels_similarity.detach().cpu())\n","            all_labels.extend(labels.detach().cpu().tolist())\n","\n","            batch_idx += 1\n","\n","            if len(all_labels) >= args.max_items:\n","                break\n","\n","    print(\"Done loop\")\n","\n","    all_features = torch.vstack(all_features).numpy()\n","    all_logits = torch.vstack(all_logits).numpy()\n","    all_labels = np.array(all_labels)\n","\n","    print(all_features.shape, all_logits.shape, all_labels.shape)\n","\n","    acc = np.sum(np.argmax(all_logits, -1) == all_labels)/all_features.shape[0]\n","    print(\"acc\", acc)\n","\n","    os.makedirs(save_path, exist_ok=True)\n","\n","    np.save(save_path+\"features.npy\", all_features)\n","    np.save(save_path+\"logits.npy\", all_logits)\n","    np.save(save_path+\"labels.npy\", all_labels)"]},{"cell_type":"code","execution_count":null,"id":"86e74818-b1e5-44e1-b4c0-1bf18c524353","metadata":{"id":"86e74818-b1e5-44e1-b4c0-1bf18c524353"},"outputs":[],"source":["@dataclass\n","class Args:\n","    dataset: str\n","    model: str=\"quilt\"\n","    seed: int=0\n","    device: str=\"cuda\"\n","    batch_size: int=32\n","    max_items: int=100000\n","    descriptors: str=None"]},{"cell_type":"code","source":["for dataset in [\n","    \"lc25000_lung\",\n","    \"lc25000_colon\",\n","    \"mhist\",\n","    \"pcam\",\n","    \"bach\",\n","    \"nck\",\n","    \"osteo\",\n","    \"renal_cell\",\n","    \"skin\",\n","    \"skin_tumor\",\n","    \"sicap\",\n","    \"databiox\"\n","]:\n","  for model in [\n","      \"plip\",\n","      \"biomed\",\n","      \"quilt\",\n","  ]:\n","    args = Args(dataset = dataset, model=model)\n","    extract_features(args)"],"metadata":{"id":"C8vybjxy9cQk"},"id":"C8vybjxy9cQk","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[{"file_id":"1ifDWGL5r0wG2n_0Dlny28yPNC4JGSkik","timestamp":1713104289016}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":5}