# On Calibration of Modern Medical Vision-Language Models

## Approach

![alt text](./figs/zero_shot_acc_vs_ece2.png)

An extensive empirical investigation across different models, datasets, and prompting
strategies showed that medical vision-language models tend to be largely unreliable out-of-the-box in the zero-shot setting.

## Setup

Requires python >= 3.10, [uncertainty calibration](https://pypi.org/project/uncertainty-calibration/) library, and [OpenAI's CLIP](https://pypi.org/project/open-clip-torch/) library.

## Acquiring Raw Data and Models

The 12 datasets used can be downloaded from:
| Dataset            | Source                                                                                  |
| ------------------ | ----------------------------------------------------------------------------------------|
| Databiox           | https://databiox.com/datasets/                                                          |
| SICAPv2            | https://data.mendeley.com/datasets/9xxm58dvs3/1                                         |
| Skin Cancer        | https://heidata.uni-heidelberg.de/dataset.xhtml?persistentId=doi:10.11588/data/7QCR8S   |
| Renal Cell         | https://zenodo.org/records/7898308                                                      |
| BACH               | https://zenodo.org/records/3632035/files/ICIAR2018_BACH_Challenge.zip                   |
| NCT-NRC            | https://zenodo.org/records/1214456/files/NCT-CRC-HE-100K.zip                            |
| PCAM               | https://patchcamelyon.grand-challenge.org/                                              |
| MHIST              | https://bmirds.github.io/MHIST/                                                         |
| Osteo              | https://www.cancerimagingarchive.net/collection/osteosarcoma-tumor-assessment/          |
| LC25000 (LUNG)     | https://huggingface.co/datasets/1aurent/LC25000                                         |
| LC2500 (Colon)     | https://huggingface.co/datasets/1aurent/LC25000                                         |

The 3 models used can be sourced from: [QuiltNet](https://github.com/wisdomikezogwo/quilt1m), [BioMedCLIP](https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224), [PLIP](https://huggingface.co/vinid/plip).

## Experiments

The experiments conducted in the study can be replicated with the following Jupyter notebooks:

### extract_features_from_datasets
After the datasets and models have been downloaded, this notebook extracts the normalised image features and the similarity between the encoded images and the relevant text embedding generated by a determined prompting strategy. 
To run this code specify the relevant dataset and model:
```
args = Args(dataset = dataset, model=model)
extract_features(args)
```

### zero_shot_experiments
This notebook calculates the accuracy, ECE1 and ECE2 for each model and dataset for zero shot classification task.
To run this code specify the metric and descriptor (prompting strategy):
```
args = Args(
    metric="ece2",
    descriptors=None
)
run_exp(args)
```

### few_shot_adaptation_exp
This notebook determines the tradeoff between calibration and accuracy given a small validation set of data. The experiments explore which base medical CLIP model and recalibration method to use as well as how to split the data between the recalibrator and training the linear model for the downstream task.
To run the code specify the dataset, model, recalibator, number of trials (n_trials) and the split size of the validation dataset (n_val)
```
args = Args(dataset = dataset, model=model, cal_fn=cal_fn, n_trials=n_trials, n_val=n_val)
df = run_exp(args)
```

### produce_class_descriptors
This notebook calls on OPENAIâ€™s chat to generate the class descriptors for each dataset. The whole notebook can be run to store results in the relevant file path. 

### ensemble_experiments
This notebook repeats the exercise in zero_shot_experiements, except the results are averaged across an ensemble of prompting strategies and models per dataset. mega_ensemble() can be called to run the code.

